# Project 1: Data Modeling with Postgres

--------------------------------------------

## Summary
The purpose of the README is to explain the Database schema, the project structure and how to run it.

* [Database schema](#Database-schema)
* [Project structure](#Project-structure)
* [How to run](#How-to-run)

--------------------------------------------

## Database schema
The database in this projects consist in a **Start Schema** with 5 tables:
* **songplays**: it is the only fact table which is contains foreign keys to song_id, artist_id, user_id and start_time.
* **songs**: dimensional table which contains songs information.
* **artists**: dimensional table which contains artists information.
* **users**: dimensional table which contains information about users in the app.
* **time**: dimensional table which contains timestamps of records in songplays broken down into specific units.

--------------------------------------------

## Project structure
This is the project structure:

* **/data** - Source of the JSON file, all these files have to be elaborated
  * **/song_data** -  This folder contains a files in JSON format with metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. This dataset is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/).
  * **/lod_data** - The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate app activity logs from a music streaming app based on specified configurations.
* **test.ipynb** - It is a notebook that displays the first few rows of each table to let you check your database.
* **create_tables.py** - This script drops old tables, if exist, and create new tables.
* **etl.ipynb** - It is a notebook that reads and processes a single file from song_data and log_data. It contains detailed instructions on the ETL process for each of the tables.
* **etl.py** - This python script reads and processes files from song_data and log_data and loads them into the tables.
* **sql_queries.py** - This file contains all the sql queries used to create the tables and ingest the data.

----------------------------

## How to run

* Install PostgreSQL and launch an instance.
* Install Python and the libraries psycopg2 and pandas for Python.
* Open a terminal session, set your filesystem on project root folder and run the following commands:
  * *python creates_tables.py*: it will drop and create your tables.
  * *python etl.py*: it will process the datasets from /data. Rembemer to always run creates_tables.py before running this script.